{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Deep Q-Network Learning_\n",
    "## with Lunar Lander by OpenAI Gym\n",
    "\n",
    "In this project, it will train an agent to land a lunar lander safely on a landing pad on the surface of the moon.\n",
    "\n",
    "---\n",
    "\n",
    "by Naufal Mu'afi\n",
    "\n",
    "_copyright to prof. Andrew NG and DeepLearning.AI_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages\n",
    "First, import all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a virtual display to render the Lunar Lander environment\n",
    "# Display(visible=0, size=(840, 480)).start()\n",
    "\n",
    "# set the random seed for TensorFlow\n",
    "tf.random.set_seed(utils.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Lunar Lander Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load The Lunar Lander Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00185976,  1.4221803 , -0.18839662,  0.5004553 ,  0.00216186,\n",
       "         0.04267462,  0.        ,  0.        ], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "# PIL.Image.fromarray(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Shape = (8,)\n",
      "Number of Actions = 4\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "print(f\"State Shape = {state_size}\")\n",
    "print(f\"Number of Actions = {num_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interacting with the Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State\t\t: (array([0.004, 1.414, 0.365, 0.125, -0.004, -0.083, 0.000, 0.000],\n",
      "      dtype=float32), {})\n",
      "Action\t\t\t: 0\n",
      "Next State\t\t: [0.043 1.405 0.365 -0.167 -0.049 -0.082 0.000 0.000]\n",
      "Reward Received\t\t: -1.0802451748785984\n",
      "Episode Terminated\t: False\n",
      "Info\t\t\t: {}\n"
     ]
    }
   ],
   "source": [
    "# select the action\n",
    "action = 0\n",
    "\n",
    "# run a single time step of the environment's dynamics with the given action\n",
    "next_state, reward, done, _, info = env.step(action)\n",
    "\n",
    "with np.printoptions(formatter={'float': '{:.3f}'.format}):\n",
    "  print(f\"Initial State\\t\\t: {initial_state}\")\n",
    "  print(f\"Action\\t\\t\\t: {action}\")\n",
    "  print(f\"Next State\\t\\t: {next_state}\")\n",
    "  print(f\"Reward Received\\t\\t: {reward}\")\n",
    "  print(f\"Episode Terminated\\t: {done}\")\n",
    "  print(f\"Info\\t\\t\\t: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Target Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Q-Network\n",
    "q_network = Sequential([\n",
    "  Input(shape=state_size),\n",
    "  Dense(units=64, activation='relu'),\n",
    "  Dense(units=64, activation='relu'),\n",
    "  Dense(units=num_actions, activation='linear')\n",
    "])\n",
    "\n",
    "# Create the Target Q-Network\n",
    "target_q_network = Sequential([    \n",
    "  Input(shape=state_size),                      \n",
    "  Dense(units=64, activation='relu'),            \n",
    "  Dense(units=64, activation='relu'),            \n",
    "  Dense(units=num_actions, activation='linear'),    \n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll tests passed!\n",
      "\u001b[92mAll tests passed!\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "from public_tests import *\n",
    "\n",
    "test_network(q_network)\n",
    "test_network(target_q_network)\n",
    "test_optimizer(optimizer, ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store expericences as named tuples\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deep Q-Learning Algorithm with Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "  \"\"\" \n",
    "    Calculates the loss.\n",
    "    \n",
    "    Args:\n",
    "      experiences       : (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma             : (float) The discount factor.\n",
    "      q_network         : (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "      target_q_network  : (tf.keras.Sequential) Karas model for predicting the targets\n",
    "          \n",
    "    Returns:\n",
    "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between the y targets and the Q(s,a) values.\n",
    "  \"\"\"\n",
    "  \n",
    "  # unpack the mini-batch of experience tuples\n",
    "  states, actions, rewards, next_states, done_vals = experiences\n",
    "  \n",
    "  # compute max Q^(s,a)\n",
    "  max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
    "  \n",
    "  # set y = R if episode terminates, otherwise set y = R + Î³ max Q^(s,a).\n",
    "  y_targets = reward + (gamma * max_qsa * (1 - done_vals))\n",
    "  \n",
    "  # get the q_values\n",
    "  q_values = q_network(states)\n",
    "  q_values = tf.gather_nd(\n",
    "    q_values,\n",
    "    tf.stack([tf.range(q_values.shape[0]),\n",
    "              tf.cast(actions, tf.int32)],\n",
    "             axis=1)\n",
    "  )\n",
    "  \n",
    "  # compute the loss\n",
    "  loss = MSE(y_targets, q_values)\n",
    "  \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Update the Network Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def agent_learn(experiences, gamma):\n",
    "  \"\"\"\n",
    "    Updates the weights of the Q networks.\n",
    "    \n",
    "    Args:\n",
    "      experiences : (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma       : (float) The discount factor.\n",
    "  \"\"\"\n",
    "  \n",
    "  # calculate the loss\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
    "  \n",
    "  # get the gradients of the loss with respect to the weights\n",
    "  gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "  \n",
    "  # update the weights of the q_network\n",
    "  optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "  \n",
    "  # update the weights of target q_network\n",
    "  utils.update_target_network(q_network, target_q_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Users\\Muafi\\Documents\\NAUFAL MU'AFI\\Universitas Gadjah Mada\\Skripsi\\learn\\demo\\DQN_lunar-lander\\main.ipynb Cell 27\u001b[0m line \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Muafi/Documents/NAUFAL%20MU%27AFI/Universitas%20Gadjah%20Mada/Skripsi/learn/demo/DQN_lunar-lander/main.ipynb#X34sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m total_points \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Muafi/Documents/NAUFAL%20MU%27AFI/Universitas%20Gadjah%20Mada/Skripsi/learn/demo/DQN_lunar-lander/main.ipynb#X34sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_num_timesteps):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Muafi/Documents/NAUFAL%20MU%27AFI/Universitas%20Gadjah%20Mada/Skripsi/learn/demo/DQN_lunar-lander/main.ipynb#X34sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m   \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Muafi/Documents/NAUFAL%20MU%27AFI/Universitas%20Gadjah%20Mada/Skripsi/learn/demo/DQN_lunar-lander/main.ipynb#X34sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m   \u001b[39m# from the current states S choose an action A using an Îµ-greedy policy  \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Users/Muafi/Documents/NAUFAL%20MU%27AFI/Universitas%20Gadjah%20Mada/Skripsi/learn/demo/DQN_lunar-lander/main.ipynb#X34sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m   state_qn \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mexpand_dims(state, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m) \u001b[39m# Expand dimensions of the numpy array        \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Muafi/Documents/NAUFAL%20MU%27AFI/Universitas%20Gadjah%20Mada/Skripsi/learn/demo/DQN_lunar-lander/main.ipynb#X34sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m   q_values \u001b[39m=\u001b[39m q_network(state_qn)    \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Muafi/Documents/NAUFAL%20MU%27AFI/Universitas%20Gadjah%20Mada/Skripsi/learn/demo/DQN_lunar-lander/main.ipynb#X34sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m   action \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mget_action(q_values, epsilon)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\shape_base.py:591\u001b[0m, in \u001b[0;36mexpand_dims\u001b[1;34m(a, axis)\u001b[0m\n\u001b[0;32m    589\u001b[0m     a \u001b[39m=\u001b[39m asarray(a)\n\u001b[0;32m    590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     a \u001b[39m=\u001b[39m asanyarray(a)\n\u001b[0;32m    593\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(axis) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    594\u001b[0m     axis \u001b[39m=\u001b[39m (axis,)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "num_episodes = 2000\n",
    "max_num_timesteps = 1000\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100  # number of total points to use for averaging\n",
    "epsilon = 1.0   # initialize Îµ value for Îµ-greedy policy\n",
    "\n",
    "# create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# set the target network weights equal to the Q-Network weights\n",
    "target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "for i in range(num_episodes):\n",
    "  \n",
    "  # reset the env to the initial state/obs space and get the initial state\n",
    "  state = env.reset()\n",
    "  total_points = 0\n",
    "  \n",
    "  for t in range(max_num_timesteps):\n",
    "    \n",
    "    # from the current states S choose an action A using an Îµ-greedy policy  \n",
    "    state_qn = np.expand_dims(state, axis=0) # Expand dimensions of the numpy array        \n",
    "    q_values = q_network(state_qn)    \n",
    "    action = utils.get_action(q_values, epsilon)\n",
    "    \n",
    "    # take action A and receive reward R and the next state S'\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    \n",
    "    # Store experience tuple (S,A,R,S') in the memory buffer.\n",
    "    # We store the done variable as well for convenience.\n",
    "    memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "    \n",
    "    # only update the network every NUM_STEPS_FOR_UPDATES time steps\n",
    "    update = utils.check_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
    "        \n",
    "    if update:\n",
    "      # Sample random mini-batch of experience tuples (S,A,R,S') from D\n",
    "      experiences = utils.get_experiences(memory_buffer)\n",
    "            \n",
    "      # Set the y targets, perform a gradient descent step,\n",
    "      # and update the network weights.\n",
    "      agent_learn(experiences, GAMMA)\n",
    "        \n",
    "    state = next_state.copy()\n",
    "    total_points += reward\n",
    "        \n",
    "    if done:\n",
    "      break\n",
    "  \n",
    "  total_point_history.append(total_points)\n",
    "  av_latest_points = np.mean(total_point_history[-num_p_av:])\n",
    "  \n",
    "  # Update the Îµ value\n",
    "  epsilon = utils.get_new_eps(epsilon)\n",
    "  \n",
    "  print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\", end=\"\")\n",
    "  \n",
    "  if (i+1) % num_p_av == 0:\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "  \n",
    "  # We will consider that the environment is solved if we get an\n",
    "  # average of 200 points in the last 100 episodes.\n",
    "  \n",
    "  if av_latest_points >= 200.0:\n",
    "    print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "    q_network.save('lunar_lander_model.h5')\n",
    "    \n",
    "    break\n",
    "\n",
    "tot_time = time.time() - start\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
